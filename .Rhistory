summary(c)
summary(c)$coefficients
sumCoef <- summary(c)$coefficients
sumCoef
sumCoef[2,1] + c(-1,1) * qt(.975, df=c$df) * sumCoef[2,2]
a <- lm(mpg~wt)
anova(a)
847.73/278.32
y <- lm(mean(wt) ~ 1)
z <- lm(mpg~wt)
y/z
y
y[1,1]
y$coefficients
y$coefficients[1,1]
y$coefficients[1]
y$coefficients[1] / (278/847)
278/847
y
y$residual
z$residual
y$residuals
z$residuals
resid(z)
sum(resid(z)^2)
sum(resid(y)^2)
yhat <- predict(fit, newdata=data.frame(wt=mtcars$wt))
ymean <- mean(wt)
sum((mtcars$mpg-yhat)^2) / sum((mtcars$mpg - ymean)^2)
anova(fit)
sum((mtcars$mpg-yhat)^2)
sum((mtcars$mpg - ymean)^2)
fit
a
yhat <- predict(fit, newdata=data.frame(wt=mtcars$wt))
yhat
sum((mpg-yhat) ^2)
ymean <- predict(fit, mean(wt))
ymean
sum((mpg-ymean)^2)
ymean <- predict(fit, newdata=data.frame(wt=mean(wt)))
ymean
sum(mpg-ymean)^2)
sum((mpg-ymean)^2)
y <- sum((mpg-yhat) ^2)
z <- sum((mpg-ymean)^2)
y/z
library(caret)
install.packages("caret")
library(caret)
library(kernlab)
install.packages("kernlab")
library(kernlab)
data(spam)
spam
inTrain <- createDataPartition(y=spam$type, p=.75, list=FALSE)
class(inTrain)
inTrain
head(inTrain)
head(spam)
spam
spam$type
unique(spam$type)
table(spam$type)
table(unique(spam$type))
exit
setwd("~/LL/Coursera/Courses/PracticalMachineLearning")
install.packages("ElemStatLearn")
library(caret)
library(gbm)
library(mgcv)
library(nlme)
install.packages("gbm")
library("AppliedPredictiveLearning")
install.packages("AppliedPredictiveLearning")
install.packages("AppliedPredictiveModeling")
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
fitRF <- train(diagnosis~., method="rf", training)
pRF <- predict(fitRF, testing)
cmRF <- confusionMatrix(pRF, testing$diagnosis)
cmRF$overall['Accuracy']
fitGBM <- train(diagnosis~., method="gbm", training)
pGBM <- predict(fitGBM, testing)
cmGBF <- confusionMatrix(pGBM, testing$diagnosis)
cmGBF$overall['Accuracy']
fitLDA <- train(diagnosis~., method="lda", training)
pLDA <- predict(fitLDA, testing)
cmLDA <- confusionMatrix(pLDA, testing$diagnosis)
cmLDA$overall['Accuracy']
combinedset <- data.frame(pRF, pGBM, pLDA, diagnosis <- testing$diagnosis)
fit <- train(diagnosis~., combinedset, method="rf")
predict <- predict(fit, combinedset)
cm <- confusionMatrix(predict, testing)
cm$overall['Accuracy']
testing
combinedset <- data.frame(pRF, pGBM, pLDA, diagnosis <- testing$diagnosis)
fit <- train(diagnosis~., combinedset, method="rf")
predict <- predict(fit, combinedset)
cm <- confusionMatrix(predict, testing$diagnosis)
cm$overall['Accuracy']
combinedset$diagnosis
identical(combinedset$diagnosis, testing$diagnosis)
combinedset <- data.frame(pRF, pGBM, pLDA, diagnosis <- testing$diagnosis)
fit <- train(diagnosis~., combinedset, method="rf")
predict <- predict(fit, testing)
cm <- confusionMatrix(predict, testing)
cm$overall['Accuracy']
combinedset <- data.frame(pRF, pGBM, pLDA, diagnosis <- testing$diagnosis)
fit <- train(diagnosis~., combinedset, method="rf")
predict <- predict(fit, testing)
cm <- confusionMatrix(predict, testing$diagnosis)
cm$overall['Accuracy']
set.seed(62433)
# create models
fit1 <- train(diagnosis ~ ., data = training, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(diagnosis ~ ., data = training, method = "gbm")
fit3 <- train(diagnosis ~ ., data = training, method = "lda")
# predict test
predict1 <- predict(fit1, newdata = testing)
predict2 <- predict(fit2, newdata = testing)
predict3 <- predict(fit3, newdata = testing)
# combine predictions
DF_combined <- data.frame(predict1, predict2, predict3, diagnosis = testing$diagnosis) # training$diagnosis?
fit_combined <- train(diagnosis ~ ., data = DF_combined, method = "rf")
predict4 <- predict(fit_combined, newdata = testing)
# confusion matrixes
c1 <- confusionMatrix(predict1, testing$diagnosis)
c2 <- confusionMatrix(predict2, testing$diagnosis)
c3 <- confusionMatrix(predict3, testing$diagnosis)
c4 <- confusionMatrix(predict4, testing$diagnosis)
print(paste(c1$overall[1], c2$overall[1], c3$overall[1], c4$overall[1]))
?trControl
(caret)
library(caret)
?trControl
?train
?trainControl
set.seed(62433)
# create models
fit1 <- train(diagnosis ~ ., data = training, method = "rf", trControl = trainControl(number = 3))
fit2 <- train(diagnosis ~ ., data = training, method = "gbm")
fit3 <- train(diagnosis ~ ., data = training, method = "lda")
# predict test
predict1 <- predict(fit1, newdata = testing)
predict2 <- predict(fit2, newdata = testing)
predict3 <- predict(fit3, newdata = testing)
# combine predictions
DF_combined <- data.frame(predict1, predict2, predict3, diagnosis = testing$diagnosis) # training$diagnosis?
fit_combined <- train(diagnosis ~ ., data = DF_combined, method = "rf")
predict4 <- predict(fit_combined, newdata = testing)
# confusion matrixes
c1 <- confusionMatrix(predict1, testing$diagnosis)
c2 <- confusionMatrix(predict2, testing$diagnosis)
c3 <- confusionMatrix(predict3, testing$diagnosis)
c4 <- confusionMatrix(predict4, testing$diagnosis)
print(paste(c1$overall[1], c2$overall[1], c3$overall[1], c4$overall[1]))
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
```
Set the seed to 62433 and predict diagnosis with all the other variables using a random forest ("rf"), boosted trees ("gbm") and linear discriminant analysis ("lda") model. Stack the predictions together using random forests ("rf"). What is the resulting accuracy on the test set? Is it better or worse than each of the individual predictions?
```{r message=FALSE, warning=FALSE}
set.seed(62433)
fitRF <- train(diagnosis~., method="rf", training, trControl=trainControl(number=3))
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
fitRF <- train(diagnosis~., method="rf", training, trControl=trainControl(number=3))
fitGBM <- train(diagnosis~., method="gbm", training)
fitLDA <- train(diagnosis~., method="lda", training)
pRF <- predict(fitRF, testing)
pGBM <- predict(fitGBM, testing)
pLDA <- predict(fitLDA, testing)
combinedset <- data.frame(pRF, pGBM, pLDA, diagnosis <- testing$diagnosis)
fit <- train(diagnosis~., combinedset, method="rf")
predict <- predict(fit, testing)
cmRF <- confusionMatrix(pRF, testing$diagnosis)
cmGBF <- confusionMatrix(pGBM, testing$diagnosis)
cmLDA <- confusionMatrix(pLDA, testing$diagnosis)
cm <- confusionMatrix(predict, testing$diagnosis)
cmRF$overall['Accuracy']
cmGBF$overall['Accuracy']
cmLDA$overall['Accuracy']
cm$overall['Accuracy']
set.seed(62433)
# create models
fit1 <- train(diagnosis ~ ., data = training, method = "rf", trControl = trainControl(number = 4))
identical(fitRF, fit1)
fitRF
fit1
set.seed(62433)
# create models
fit1 <- train(diagnosis ~ ., data = training, method = "rf", trControl = trainControl(number = 3))
identical(fitRF, fit1)
fitRF
fit1
fitRF <- train(diagnosis~., data = training, method="rf", trControl=trainControl(number=3))
identical(fitRF, fit1)
fit1 <- train(diagnosis ~ ., data = training, method = "rf", trControl = trainControl(number = 3))
fita <- train(diagnosis ~ ., data = training, method = "rf", trControl = trainControl(number = 3))
identical(fit1, fita)
fitGBM <- train(diagnosis~., method="gbm", training)
fit2 <- train(diagnosis ~ ., data = training, method = "gbm")
fitGBM
fit2
DF_combined <- data.frame(predict1, predict2, predict3, diagnosis = testing$diagnosis
)
combinedset <- data.frame(pRF, pGBM, pLDA, diagnosis <- testing$diagnosis)
identical(DF_combined, combinedset)
cmRF$overall['Accuracy']
c1$overall[1]
cm$overall['Accuracy']
c4
cm
cmRF
nrow(adData)
pRF
nrow(pRF)
class(pRF)
length(pRF)
combineddata
combinedset
head(combinedset)
combinedset <- data.frame(pRF, pGBM, pLDA, diagnosis <- testing$diagnosis)
combinedseta <- data.frame(pRF, pGBM, pLDA, diagnosis = testing$diagnosis)
head(combinedseta)
nrow(combinedset)
nrow(combinedseta)
combinedset$diagnosis
combinedseta$diagnosis
identical(combinedset, combinedseta)
identical(combinedset$diagnosis, combinedseta$diagnosis)
names(combinedset)
combinedset <- data.frame(pRF, pGBM, pLDA, diagnosis = testing$diagnosis)
fit <- train(diagnosis~., combinedset, method="rf")
predict <- predict(fit, testing)
cm <- confusionMatrix(predict, testing$diagnosis)
cm
cmGBF
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
library(caret)
library(gbm)
library(mgcv)
library(nlme)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
rf <- train(y~., method="rf", data=vowel.train)
gbm <- train(y~., method="gbm", data=vowel.train)
prf <- predict(rf, vowel.test)
pgbm <- predict(gbm, vowel.test)
cmrf <- confusionMatrix(prf, vowel.test$y)
cmgbm <- confusionMatrix(pgbm, vowel.test$y)
cmrf$overall['Accuracy']
cmgbm$overall['Accuracy']
datacombined <- data.frame(prf, pgbm, y=vowel.test$y)
fit <- train(y~., datacombined, method="gam")
pfit <- predict(fit, vowel.test)
cmpfit <- confusionMatrix(pfit, vowel.test$y)
cmpfit$overall['Accuracy']
datacombined
pfit
fit
?train
fit <- train(y~., datacombined)
pfit <- predict(fit, vowel.test)
cmpfit <- confusionMatrix(pfit, vowel.test$y)
cmpfit$overall['Accuracy']
?plot.enet
?plot.enet()
install.packages("elasticnet")
library(elasticnet)
?plot.enet
set.seed(233)
fit <- train(CompressiveStrength ~ ., training, method="lasso")
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
?plot.enet
set.seed(233)
fit <- train(CompressiveStrength ~ ., training, method="lasso")
fit
plot.enet(fit)
fit$finalModel
plot.enet(fit, xvar="peenalty")
plot.enet(fit, xvar="penalty")
plot.enet(fit$finalModel, xvar="penalty")
plot.enet(fit$finalModel, xvar="penalty", color=TRUE)
?plot.enet
plot.enet(fit$finalModel, xvar="penalty", use.color=TRUE)
?plot.enet
plot.enet(fit$finalModel, xvar="penalty", use.color=TRUE)
legend('topright', names(fit))
plot.enet(fit$finalModel, xvar="penalty", use.color=TRUE)
legend('topright')
?legend
plot.enet(fit$finalModel, xvar="penalty", use.color=TRUE)
legend('topright', fit$finalModel)
plot.enet(fit$finalModel, xvar="penalty", use.color=TRUE)
legend('topright', fit$finalModel, lty=1)
library(lubridate)  # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
install.packages("forecast")
install.packages("quantmod")
library(forecast)
library(quantmod)
library(lubridate)  # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
library(forecast)
library(quantmod)
library(lubridate)  # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
library(forecast)
library(quantmod)
library(lubridate)
dat = read.csv("~/Desktop/gaData.csv")
dat = read.csv("~/Desktop/gaData.csv")
dat = read.csv("~/Desktop/gaData.csv")
setwd("~/LL/Coursera/Courses/PracticalMachineLearning")
dat = read.csv("~/LL/Coursera/Courses/PracticalMachineLearning/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
View(dat)
dat
mod <- bats(tstrain)
mod
mod[1]
dim(mod)
fcast <- forecast(mod)
plot(fcast)
accuracy(fcast, testing).
accuracy(fcast, testing)
accuracy(fcast, testing$visitsTumblr)
?accuracy
?fcast
?forecast
fcast <- forecast(mod, level=95)
accuracy(fcast, testing$visitsTumblr)
acc <- accuracy(fcast, testing$visitsTumblr)
acc$overall['Accuracy']
acc$overall
acc
fcast
testing
fcast <- forecast(mod, level=95)
fcast
mod
?bats
tstrain
training
tstrain
testing
head(training)
training
fcast
tstrain
tstrain
?forecast.bats
dim(testing)[1]
testing
testing[1]
length(testing[1])
dim(testing[1])
class(testing)
head(testing)
names(testing)
testing[1]
dim(testing)
testing[2]
testing[3]
testing[1]
testing
View(testing)
nrow(testing)
mod <- bats(tstrain)
fcast <- forecast(mod, level=95, h=nrow(testing))
acc <- accuracy(fcast, testing$visitsTumblr)
acc
fcast
nrow(fcast)
class(fcast)
head(fcast)
fcast
fcast <- forecast.bats(mod, level=95, h=nrow(testing))
acc <- accuracy(fcast, testing$visitsTumblr)
fcast
acc
str(acc)
str(fcast)
fcast$upper
fcast$lower
fcast
acc
fit <- bats(tstrain)
h <- dim(testing)[1]
fcast <- forecast(fit, level = 95, h = h)
accuracy(fcast, testing$visitsTumblr)
result <- c()
l <- length(fcast$lower)
for (i in 1:l){
x <- testing$visitsTumblr[i]
a <- fcast$lower[i] < x & x < fcast$upper[i]
result <- c(result, a)
}
sum(result)/l * 100
fcast
?c
testing
mod <- bats(tstrain)
fcast <- forecast.bats(mod, level=95, h=nrow(testing))
acc <- accuracy(fcast, testing$visitsTumblr)
count <- 0
for (i in 1:nrow(testing)) {
if (acc$lower < testing$visitsTumblr < acc$upper) {
count <- count + 1
}
}
count/nrow(testing)
if (acc$lower < testing$visitsTumblr < acc$upper) {
count <- count + 1
}
acc$lower
acc
fcast$lower
mod <- bats(tstrain)
fcast <- forecast.bats(mod, level=95, h=nrow(testing))
acc <- accuracy(fcast, testing$visitsTumblr)
count <- 0
for (i in 1:nrow(testing)) {
if (fcast$lower < testing$visitsTumblr < fcast$upper) {
count <- count + 1
}
}
count/nrow(testing)
if (fcast$lower < testing$visitsTumblr < fcast$upper) {
count <- count + 1
}
if (fcast$lower < testing$visitsTumblr & testing$visitsTumblr < fcast$upper) {
count <- count + 1
}
count <- 0
for (i in 1:nrow(testing)) {
if (fcast$lower < testing$visitsTumblr) {
if(testing$visitsTumblr < fcast$upper) {
count <- count + 1}
}
}
count
fit <- bats(tstrain)
# check how long the test set is, so you can predict beyond trainign
h <- dim(testing)[1]
# forecast the model for remaining time points
fcast <- forecast(fit, level = 95, h = h)
# get the accuracy
accuracy(fcast, testing$visitsTumblr)
count <- 0
for (i in 1:nrow(testing)) {
if (testing$visitsTumblr > fcast$lower) {
if(testing$visitsTumblr < fcast$upper) {
count <- count + 1}
}
}
count
testing$visitsTumblr[1]
mod <- bats(tstrain)
fcast <- forecast.bats(mod, level=95, h=nrow(testing))
acc <- accuracy(fcast, testing$visitsTumblr)
count <- 0
for (i in 1:nrow(testing)) {
if (testing$visitsTumblr[i] > fcast$lower[i]) {
if(testing$visitsTumblr[i] < fcast$upper[i]) {
count <- count + 1}
}
}
count/nrow(testing)
